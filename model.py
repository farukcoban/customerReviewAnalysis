from pyspark.sql import SparkSession
from pyspark.sql.functions import col, when, split, size, year, concat_ws
from pyspark.ml.feature import Tokenizer, StopWordsRemover, CountVectorizer
from pyspark.ml.classification import LogisticRegression
from pyspark.ml.evaluation import BinaryClassificationEvaluator
from pyspark.ml import Pipeline

# ğŸš€ Spark session baÅŸlatma (Bellek optimizasyonu yapÄ±ldÄ±)
spark = SparkSession.builder.appName("SentimentAnalysisWithMLlib") \
    .config("spark.sql.shuffle.partitions", "8") \
    .config("spark.executor.memory", "4g") \
    .config("spark.driver.memory", "2g") \
    .config("spark.executor.cores", "2") \
    .config("spark.yarn.executor.memoryOverhead", "512") \
    .config("spark.local.dir", "/tmp/spark-temp") \
    .config("spark.sql.warehouse.dir", "hdfs:///user/hive/warehouse") \
    .config("spark.history.fs.logDirectory", "hdfs:///user/spark/job-history/") \
    .config("spark.eventLog.dir", "hdfs:///user/spark/event-logs/") \
    .getOrCreate()

# ğŸ“Œ Veri seti ve sonuÃ§larÄ±n kaydedileceÄŸi yol
input_path = "gs://farkcbsbi/Electronics.jsonl.gz"
output_path = "gs://farkcbsbi/model_results.csv"

# ğŸ“Œ 1. Veri YÃ¼kleme ve Ã–n Ä°ÅŸleme
df = spark.read.json(input_path)
df = df.repartition(10)  # BÃ¼yÃ¼k veri iÅŸleme iÃ§in paralelliÄŸi artÄ±r

# YalnÄ±zca gerekli sÃ¼tunlarÄ± seÃ§me ve eksik verileri filtreleme
df = df.select("text", "rating", "verified_purchase").filter(col("text").isNotNull() & col("rating").isNotNull())

# PuanlarÄ± pozitif (1) ve negatif (0) olarak etiketleme
df = df.withColumn("label", when(col("rating") >= 4, 1).otherwise(0))

# ğŸ“Œ 2. Metin Ã–n Ä°ÅŸleme
# Tokenizer: Kelimelere ayÄ±rma
tokenizer = Tokenizer(inputCol="text", outputCol="words")

# StopWordsRemover: Gereksiz kelimeleri Ã§Ä±karma
stopwords_remover = StopWordsRemover(inputCol="words", outputCol="filtered_words")

# CountVectorizer: Kelimeleri vektÃ¶rleÅŸtirme (Optimizasyon: Kelime sayÄ±sÄ±nÄ± sÄ±nÄ±rla)
vectorizer = CountVectorizer(inputCol="filtered_words", outputCol="features", vocabSize=5000, minDF=5)

# Pipeline oluÅŸturma
pipeline = Pipeline(stages=[tokenizer, stopwords_remover, vectorizer])
processed_df = pipeline.fit(df).transform(df).cache()

# ğŸ“Œ 3. EÄŸitim ve Test Setlerine AyÄ±rma
train_df, test_df = processed_df.randomSplit([0.8, 0.2], seed=42)

# ğŸ“Œ 4. Model EÄŸitimi
lr = LogisticRegression(featuresCol="features", labelCol="label", maxIter=20, regParam=0.1)
lr_model = lr.fit(train_df)

# ğŸ“Œ 5. Model DeÄŸerlendirme
predictions = lr_model.transform(test_df).cache()
evaluator = BinaryClassificationEvaluator(labelCol="label", metricName="areaUnderROC")
auc = evaluator.evaluate(predictions)
print(f"ğŸ“Š Model AUC: {auc}")

# ğŸ“Œ 6. CSV Kaydetme Ä°Ã§in `words` SÃ¼tununu String'e Ã‡evirme
predictions = predictions.withColumn("words", concat_ws(" ", col("words")))

# ğŸ“Œ 7. SonuÃ§larÄ±n Kaydedilmesi (Optimizasyon: Daha az bÃ¶lÃ¼nmÃ¼ÅŸ dosya ile kaydet)
predictions.coalesce(1).write.csv(output_path, header=True, mode="overwrite")
print(f"âœ… Tahmin sonuÃ§larÄ± ÅŸu adrese kaydedildi: {output_path}")
